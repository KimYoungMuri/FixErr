Traceback (most recent call last):                                            
  File "/Users/youngkim/cursorprojects/transformer/train.py", line 92, in <module>
    main()
    ~~~~^^
  File "/Users/youngkim/cursorprojects/transformer/train.py", line 83, in main
    trainer.train(
    ~~~~~~~~~~~~~^
        num_epochs=config.num_epochs,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        gradient_accumulation_steps=1
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/youngkim/cursorprojects/transformer/training/trainer.py", line 87, in train
    outputs = self.model(**batch)
  File "/Users/youngkim/cursorprojects/transformer/venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/Users/youngkim/cursorprojects/transformer/venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/youngkim/cursorprojects/transformer/models/transformer_model.py", line 136, in forward
    hidden_states = encoder_layer(hidden_states, attention_mask)
  File "/Users/youngkim/cursorprojects/transformer/venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/Users/youngkim/cursorprojects/transformer/venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/youngkim/cursorprojects/transformer/models/transformer_model.py", line 223, in forward
    attention_output = self.attention(hidden_states, attention_mask)
  File "/Users/youngkim/cursorprojects/transformer/venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/Users/youngkim/cursorprojects/transformer/venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/youngkim/cursorprojects/transformer/models/transformer_model.py", line 343, in forward
    attention_scores = attention_scores + attention_mask
                       ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (512) must match the size of tensor b (8) at non-singleton dimension 2
